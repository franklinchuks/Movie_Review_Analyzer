{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Movie Reviews from IMDb:\n",
    "\n",
    "#You can use libraries like BeautifulSoup and requests to scrape movie reviews from IMDb. Here's a simplified example:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_imdb_reviews(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    reviews = soup.find_all('div', class_='text')\n",
    "    return [review.get_text() for review in reviews]\n",
    "\n",
    "imdb_url = 'https://www.imdb.com/title/tt0111161/reviews'\n",
    "reviews = scrape_imdb_reviews(imdb_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\frank\\Downloads\\tmdb_5000_credits.csv\\blank2.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/frank/Downloads/tmdb_5000_credits.csv/blank2.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import libraries\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/frank/Downloads/tmdb_5000_credits.csv/blank2.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/frank/Downloads/tmdb_5000_credits.csv/blank2.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/frank/Downloads/tmdb_5000_credits.csv/blank2.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#Text Preprocessing, Tokenization, and Lemmatization:\n",
    "#Use NLTK or other NLP libraries for text preprocessing, tokenization, and lemmatization.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "preprocessed_reviews = [preprocess_text(review) for review in reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization:\n",
    "\n",
    "\n",
    "# You can use Scikit-learn's TF-IDF vectorizer to convert the preprocessed text into numerical features.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(preprocessed_reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data:\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Evaluate ML Models:\n",
    "\n",
    "#Train and evaluate various ML models using Scikit-learn and TensorFlow.\n",
    "\n",
    "#Here's an example using a simple logistic regression model:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Neural Network:\n",
    "\n",
    "#For a neural network with LSTM layers, you can use TensorFlow and Keras.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(preprocessed_reviews)\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "# Build and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=100))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test_padded, y_test)[1]\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization:\n",
    "\n",
    "#You can use Matplotlib, Seaborn, and WordCloud to visualize your results and insights based on the model's predictions and the original reviews.\n",
    "\n",
    "#This is a simplified example of building a sentiment analysis model. In practice, you may need more data, hyperparameter tuning, and a more robust model architecture for better results.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
